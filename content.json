{"meta":{"title":"学习/分享/记录","subtitle":"","description":"个人小站点","author":"张xiao博","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"Andorid Graphics学习","slug":"Android/Graphics/Graphics","date":"2022-01-18T07:34:11.000Z","updated":"2022-01-18T09:57:43.065Z","comments":true,"path":"2022/01/18/Android/Graphics/Graphics/","link":"","permalink":"http://example.com/2022/01/18/Android/Graphics/Graphics/","excerpt":"","text":"Graphics1. Android 绘制API1.1 提供了图像渲染API可以用来绘制2D和3D,开发者绘制图像的三种方式,Canvas,OpenGLES,Vulkan。前两种我使用的较多. 1.2 例如：曾经使用TFLite对同一个物体中，不同位置进行识别后，然后使用Canvas对不同位置进行2D贴图，以实现在真实图体中浮现AI识别的功能。 1.3 例如：在相机纹理，滤镜处理中使用OpenGLES以及GLSL对相机的原始纹理进行处理后再次显示，同时在纹理中进行贴图处理，使用OpenGLES可满足 1.2 绘制流程从系统图像渲染角度来看，无论使用哪个API，一切内容均会以Surface进入缓冲区队列，最终被SurfaceFlinger消耗合成屏幕 2.1 生产方 OpenGLES Canvas mediaServer视频解码器 2.2 消费者 SurfaceFlinger，消费可见的Surface,并通过窗口管理器中的信息将其合成到屏幕 2.3 SurfaceFlinger 使用OpenGLES进行缓冲区合成后 通过Hardware Compose执行后面工作，作为渲染的中心点 1.3 数据流3.1 BufferQueue 作为不同组件间的粘合剂，组成方式为一对队列. 3.2 不同的渲染器(主屏幕,状态栏,系统界面)，SurfaceFligner 是合成器, HwComposer是合成器 1.4 BufferQueue的模式4.1 同步模式 默认模式，不舍弃任何缓冲区，阻塞并等待可用缓冲区 4.2 非阻塞模式，生成错误 4.3 舍弃模式 2. 图形组件2.1 SurfaceView和GLSurfaceView2.1.1 GLSurfaceView平常使用的较多，可以通过自定义Render来进行调整画面比例，通过自定义Shader调整各种特效等。官方的解释：GLSurfaceView提供了用于管理EGL上下方(系统封装好),线程见通信,与Activity声明周期交互的辅助类 2.1.2 SurfaceView的View组件由SurfaceFlinger合成，与应用界面隔离。这也就是它是单独绘制出来的，与Activity的绘制无关。[SurfaceView](https://source.android.google.cn/devices/graphics/arch-sv-glsv?hl=zh-cn) 2.2 SurfaceTexture2.2.1 SurfaceTexture 将 Surface 和 GLES 纹理相结合来创建 BufferQueue，而您的应用是 BufferQueue 的消费者。我经常使用的方式为：创建一个外部纹理ID后，通过SurfaceText将其纹理绑定，并生成Surface作为Target.这也就是提到的作为BufferQueue消费者。经常在GLSurfaceView以及Render中作为刷新onDraw()作用。 外部纹理OES与传统纹理区别Texture2D 外部纹理直接在从 BufferQueue 接收的数据中渲染纹理多边形。 外部纹理渲染程序的配置与传统的 GLES 纹理渲染程序不同。 外部纹理不一定可以执行所有传统的 GLES 纹理活动 2.3 TextureView2.3.1 TextureView 结合了 View 和 SurfaceTexture。TextureView 对 SurfaceTexture 进行包装，并负责响应回调以及获取新的缓冲区. 2.3.2 接受缓冲区的内容作为数据源，然后根据View的状态后，渲染在对应的位置 3. SurfaceFlinger SurfaceFlinger 接受缓冲区，对它们进行合成，然后发送到屏幕。WindowManager 为 SurfaceFlinger 提供缓冲区和窗口元数据，而 SurfaceFlinger 可使用这些信息将 Surface 合成到屏幕。","categories":[],"tags":[]},{"title":"线程","slug":"java/lang/Thread","date":"2022-01-17T03:03:03.000Z","updated":"2022-01-17T03:14:06.549Z","comments":true,"path":"2022/01/17/java/lang/Thread/","link":"","permalink":"http://example.com/2022/01/17/java/lang/Thread/","excerpt":"","text":"1. Thread1. A thread is a thread of execution in a program,The java Virtual Machine allows an application to have multiple threads of execution running concurrenttly java虚拟机允许一个应用可以存在多个线程 2. Thread 实现的接口Runnable,属性 name//线程名称 volatile String priority//优先级(1,5,10) 默认是5 target//执行目标 3. 面试中经常询问的问题sleep(),睡眠，不会释放锁","categories":[{"name":"Java源码","slug":"Java源码","permalink":"http://example.com/categories/Java%E6%BA%90%E7%A0%81/"}],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"http://example.com/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"使用GLSurfaceView+MediaCodec播放音视频","slug":"Android/MediaCodec/GLSurface+MediaCodec","date":"2022-01-14T09:24:06.000Z","updated":"2022-01-15T08:20:35.084Z","comments":true,"path":"2022/01/14/Android/MediaCodec/GLSurface+MediaCodec/","link":"","permalink":"http://example.com/2022/01/14/Android/MediaCodec/GLSurface+MediaCodec/","excerpt":"","text":"此部分仍有相关知识未理清，暂时仅用作记录，后续需要完善1. 创建GLSurfaceView以及自定义Render GLSurfaceView无特殊，自定义Render这里需要留意需要公开方法返回一个Surface. Surface的创建与通过GLSurfaceView显示相机画面的套路一模一样， 1.1 创建OES纹理ID 1.2 绑定OES_ID到 SurfaceTexture上 1.3 SurfaceTexture设置setOnFrameAvailableListener，用以调用刷新glSurfaceView.requsetRender() 2. MediaCodec使用(音视频使用相同策略) 2.2 初始化Extractor 2.2 创建MediaCodec 2.3 设置Surface到MediaCodec中，开始解码 2.4 将Extractor获取的数据压入解码器的缓冲区，将缓冲区的数据拉取渲染，释放当前缓冲区 2.5 循环2.4步，直至获取EOS结束标志 2.6 结束 3. 效果 将相机FBO纹理通过EGL(EGLSurface)和MediaCodec编码后生成mp4文件，然后使用GLSurface+MediaCodec播放 4.0 其他知识EGLOpenGLMediaCodecMediaCodec参照资料","categories":[{"name":"OpenGLES MediaCodec","slug":"OpenGLES-MediaCodec","permalink":"http://example.com/categories/OpenGLES-MediaCodec/"}],"tags":[{"name":"MediaCodec","slug":"MediaCodec","permalink":"http://example.com/tags/MediaCodec/"}]},{"title":"Zxing 128条码识别","slug":"Android/Zxing/128","date":"2022-01-14T08:49:31.000Z","updated":"2022-01-16T08:44:47.602Z","comments":true,"path":"2022/01/14/Android/Zxing/128/","link":"","permalink":"http://example.com/2022/01/14/Android/Zxing/128/","excerpt":"","text":"Zxing官方地址,我这里下载的的ZXing 3.4.1 版本，下载解压后需要将其中几个模块分别导入Android Studio工程，同时修改build.gralde的引用便可以编译Android 测试APK.，我这里仅琢磨了108条码的识别，查看源码逻辑后，发现其他编码的解码与编码使用的相同的思路。 1. 打开相机(简单描述) 1.1 com.google.zxing.client.android.PreviewCallback,获取 Android Camera接口获取相机数据 1.2 在同包名的CaptureActivityHandler中 将DecodeThread.getHandler传递给 PreviewCallback,以此来将相机数据传递给DecodeThread 1.3 DecodeThread构造方法中负责配置decodeFormat也就是识别的码值类型 1.4 DecodeHandler的handleMessage()，到这里所有的相机数据便进入识别 2. Decode数据处理2.1 Decode 获取扫码框内的数据YUV,并将其封装PlanarYUVLuminanceSource对象 123456789101112131415161718192021222324PlanarYUVLuminanceSource source = activity.getCameraManager().buildLuminanceSource(data, width, height); * This object extends LuminanceSource around an array of YUV data returned from the camera driver, * with the option to crop to a rectangle within the full data. This can be used to exclude * superfluous pixels around the perimeter and speed up decoding. * * It works for any pixel format where the Y channel is planar and appears first, including * YCbCr_420_SP and YCbCr_422_SP.封装YUV数据，分离Y通道 //因为是108条码识别，本质上只需要截取任意一行，根据点的分布即可 @Override public byte[] getRow(int y, byte[] row) &#123; if (y &lt; 0 || y &gt;= getHeight()) &#123; throw new IllegalArgumentException(&quot;Requested row is outside the image: &quot; + y); &#125; int width = getWidth(); if (row == null || row.length &lt; width) &#123; row = new byte[width]; &#125; int offset = (y + top) * dataWidth + left; System.arraycopy(yuvData, offset, row, 0, width); return row; &#125; 2.2 使用source，new HybridBinarizer(source)，构造HyBridBinarizer类，其父类GlobalHistogramBinarizer中便是关键，也是解码的主要操作 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic BitArray getBlackRow(int y, BitArray row) throws NotFoundException &#123; LuminanceSource source = getLuminanceSource(); int width = source.getWidth(); if (row == null || row.getSize() &lt; width) &#123; row = new BitArray(width); &#125; else &#123; row.clear(); &#125; initArrays(width);//初始化BitArray byte[] localLuminances = source.getRow(y, luminances); savelocalLuminances(localLuminances);//此处为自己添加 int[] localBuckets = buckets; for (int x = 0; x &lt; width; x++) &#123;//锐化后构建颜色直方图 localBuckets[(localLuminances[x] &amp; 0xff) &gt;&gt; LUMINANCE_SHIFT]++; &#125; //获取直方图中第一高点，第二高点 以及2点之间的波谷 int blackPoint = estimateBlackPoint(localBuckets); if (width &lt; 3) &#123; // Special case for very small images for (int x = 0; x &lt; width; x++) &#123; if ((localLuminances[x] &amp; 0xff) &lt; blackPoint) &#123; row.set(x); &#125; &#125; &#125; else &#123;//使用一个-1 4 -1 过滤对数据进行锐化 int left = localLuminances[0] &amp; 0xff; int center = localLuminances[1] &amp; 0xff; for (int x = 1; x &lt; width - 1; x++) &#123; int right = localLuminances[x + 1] &amp; 0xff; // A simple -1 4 -1 box filter with a weight of 2. if (((center * 4) - left - right) / 2 &lt; blackPoint) &#123; row.set(x); &#125; left = center; center = right; &#125; &#125; return row;&#125; 如下流程有待完善，等待后期再次更新 2.3 解码Code128Reader解码BitArray,可以看到108条码有三种类型 3. 结论3.1 解码过程有待细化以及对算法进行调整 3.2 条形码的矩形框依赖于人工，可通过TFLite 对象检测来进行识别，自动送入解码过程 3.3 源码中的实现我自己测试不是响应很快，有时间需要再看看 3.4 BitArray转编码过程还未调试，需要再次阅读源码更新","categories":[{"name":"Android Zxing","slug":"Android-Zxing","permalink":"http://example.com/categories/Android-Zxing/"}],"tags":[{"name":"Zxing","slug":"Zxing","permalink":"http://example.com/tags/Zxing/"}]},{"title":"Android 多人脸贴图","slug":"Android/OpenGLES/OpenGLES_Texture","date":"2022-01-14T08:42:25.000Z","updated":"2022-01-15T07:27:12.796Z","comments":true,"path":"2022/01/14/Android/OpenGLES/OpenGLES_Texture/","link":"","permalink":"http://example.com/2022/01/14/Android/OpenGLES/OpenGLES_Texture/","excerpt":"","text":"1. 效果图 2. 前置条件1. 通过GLSurfaceView 以及自定义Render(OES纹理)绘制相机画面，同时创建FBO纹理,用于进行贴图 2. 同时通过ImageReader读取相机原画面，传入人脸关键点识别，获取关键特征点 3. 人脸关键点以及偏转角度计算3.1 选取关键点，计算顶点坐标 我们可以选取人脸特点中具备对称的2个点。出现头部方向旋转时，可以通过这两个点来计算出头部倾斜的夹角。 然后利用倾斜角便可以计算出运算后四个顶点坐标 3.2 绘制 glViewport(0,0,width,height)//这里的绘制的GL宽高仍然为全屏幕 贴图的顶点坐标由3.1获取,实现2D贴图绘制即可。这里注意的是使用的FBO纹理，Y轴坐标获取后都需要进行反转 4. 上传2D纹理贴图 此处省略 5. 其他实现思路 5.1 定义glViewPort(x,y,widht,height)，直接将其设置为贴图的位置。存在问题：由于此处设置的矩形，在旋转时，便会出现矩形某个方向拉伸变形问题。 5.2 仅定义左下，右下顶点，(左上右上均在2者基础上加入图片高度)，通过Matrix矩阵进行旋转，通过测试结果显示会产生形变，效果不好。","categories":[{"name":"OpenGLES Android Shader","slug":"OpenGLES-Android-Shader","permalink":"http://example.com/categories/OpenGLES-Android-Shader/"}],"tags":[{"name":"OpenGLES","slug":"OpenGLES","permalink":"http://example.com/tags/OpenGLES/"}]},{"title":"开发Android_Camera_App","slug":"Android/Camera2/CameraAPP","date":"2022-01-08T12:47:44.000Z","updated":"2022-01-14T09:31:07.842Z","comments":true,"path":"2022/01/08/Android/Camera2/CameraAPP/","link":"","permalink":"http://example.com/2022/01/08/Android/Camera2/CameraAPP/","excerpt":"","text":"1.CameraAPP 1.0.1 版本1.交互区域1.1 单个人 脸区域美颜 1.2 多人头部贴图功能 1.3 单个人 大眼功能 2.保存图片","categories":[{"name":"Android Camera2 APP","slug":"Android-Camera2-APP","permalink":"http://example.com/categories/Android-Camera2-APP/"}],"tags":[{"name":"APP","slug":"APP","permalink":"http://example.com/tags/APP/"}]},{"title":"编译dlib For Android","slug":"Face/dlib","date":"2022-01-08T07:53:47.000Z","updated":"2022-01-09T07:41:32.243Z","comments":true,"path":"2022/01/08/Face/dlib/","link":"","permalink":"http://example.com/2022/01/08/Face/dlib/","excerpt":"","text":"1. Dlib基础偶然看到dlib中有个模块可以对人脸进行68个标记,所以在github上寻找可以在Android平台使用的库。参照[Luca96/android-face-landmarks]已经实现，通过对编译脚本，JNI接口调整，以及对OpenCV 头文件依赖的版本修改，实现了将dlib 19.16 +OpenCV4.5.2整合，同时可以单独编译成library. 已实现工程结构如下：,注意：此时asset工程未放入models文件。models,在这里下载model文件导入后，运行。不同的model文件对应不同的位置的表姐。 2. Dlib依赖OpenCV库升级原作者(Luca96)在编译基于dlib库生成dlibface-lib时，使用的OpenCV版本为4.0.1,由于我的Android工程使用的OpenCV SDK 版本为4.5.2，所以需要对face_dlib工程中，dlib的头文件基于OpenCV编写的头文件进行调整实现。 文件路径face_dlib/src/main/cppLibs/dlib/opencv/cv_image.h，此处调整的实现为OpenCV中Mat与IplImage转换，需要方法实现，老版本可以直接转。此文件中包含2处，均需要进行修改替换。 3. 调整Cmake构建脚本原工程中的OpenCV依赖以及构建脚本为特定目录，此处借鉴OpenCV SDK中官方例子，进行调整（整个工程依赖唯一一个module OpenCV SDK) 4. 调整JNI方法实现原实现JNI接口这里传入的yuvFrame,以及旋转角度等数据，我进行修改。传入的Mat的地址索引，旋转角度已经通过OpenCV.Core类进行了处理(人脸检测时已处理)，这里仅传入灰度数据即可。 5. 运行结果其他：绿色标注为OpenCV Face Detector以及 FaceAlignmentSeeta 6. 参照Luca96/android-face-landmarks,此博主提供了dlib编译交叉编译的步骤，由于我此时Ubuntu环境已破坏，所以直接使用了已经预编译的so文件。dlib=models,支持多个model，应对不同的场景（鼻子，嘴巴）等Luca96/dlib-for-android","categories":[{"name":"FaceLandmark OpenCV","slug":"FaceLandmark-OpenCV","permalink":"http://example.com/categories/FaceLandmark-OpenCV/"}],"tags":[{"name":"FaceLandmark","slug":"FaceLandmark","permalink":"http://example.com/tags/FaceLandmark/"}]},{"title":"编译SeetaFace人脸5个关键点库","slug":"Face/SeetaFace","date":"2022-01-08T07:52:43.000Z","updated":"2022-01-09T02:25:26.871Z","comments":true,"path":"2022/01/08/Face/SeetaFace/","link":"","permalink":"http://example.com/2022/01/08/Face/SeetaFace/","excerpt":"","text":"1. 背景疫情期间，歇业在家，偶遇github工程，便将其调整单独的Android的library库，调整JNI层接口以及内部方法实现。 工程结构图硬件：Huaiwe Mate 20 Pro 前置摄像头软件：Camera2+SeetaFaceAlignment+OpenCV 2. 调整Cmake编译环境以及构建脚本借鉴于OpenCV FaceDetection，调整Cmake编译脚本，引入OpenCV依赖，并引入FaceAlignment依赖，生成libseeta_fa_lib.so文件 3. 调整Native接口进行重新编写后，提供2个Native方式，方法1初始化FaceAlignment,并返回索引方法2，传入 FaceAlignment索引 face(OpenCV FaceDetector 人脸区域) OpenCV Mat(灰度) 第二个参数的使用，此处便是JNI将java对象传递给Native，先通过类名反射寻找到类，以及生成类对应的字段。然后通过getIntField(Object,x)获取Value.此处值得学习第三个参数借鉴于OpenCV SDK,中直接将Mat的索引地址传入，简洁快捷 4. 运行结果人脸的区域识别依赖于OpenCV FaceDetector人脸的关键点，眼睛，嘴角的识别不够准确，通过相机识别多帧，晃动较大 5. 参照SeetaFaceEngine2Android-OpenGL_Filter","categories":[{"name":"FaceLandmark OpenCV","slug":"FaceLandmark-OpenCV","permalink":"http://example.com/categories/FaceLandmark-OpenCV/"}],"tags":[{"name":"SeetaFace","slug":"SeetaFace","permalink":"http://example.com/tags/SeetaFace/"}]},{"title":"OpenCV FaceDetector 人脸检测","slug":"OpenCV/FaceDetector","date":"2022-01-07T01:48:49.000Z","updated":"2022-01-08T08:37:17.488Z","comments":true,"path":"2022/01/07/OpenCV/FaceDetector/","link":"","permalink":"http://example.com/2022/01/07/OpenCV/FaceDetector/","excerpt":"","text":"1. OpenCV For AndroidOpenCV 官网地址 https://opencv.org/releases/ ,提供了Android 平台使用的SDK SDK For Android 均包含在官方提供的opencv moudle中。 2. 官方例子face-detection(仅分析Native实现)2.1 face-detection 默认使用了opencv Camera1 (org.opencv.android.JavaCameraView)，用来获取和显示数据，可以在face_detect_surface_view.xml中对View组件进行替换为如下 1234&lt;org.opencv.android.JavaCamera2View android:layout_width=&quot;fill_parent&quot; android:layout_height=&quot;fill_parent&quot; android:id=&quot;@+id/fd_activity_surface_view&quot; /&gt; 通过替换，可以使用Camera2中的接口来获取数据和显示View 2.2 编译和构建步骤2.2.1 通过在face-detection模块的build.gradle配置Cmake编译时2个参数 OpenCV_DIR //OpenCV的路径 作用：提供编译c代码时，需要用到OpenCV的头文件 位置：sdk/native/jni/include 以及需要引入的OpenCV依赖库 targets &quot;detection_based_tracker&quot; face-detection 默认引用opencv module，在通过构建脚本CMakeList.txt 编译os文件 2.2.2 CmakeCmake构建脚本，包含引入OpenCV，以及本仓库需要编译的cpp文件 1234567891011cmake_minimum_required(VERSION 3.6)set(target detection_based_tracker)project($&#123;target&#125; CXX) #已通过build.gradle中手动添加的targetset(ANDROID_OPENCV_COMPONENTS &quot;opencv_java&quot; CACHE STRING &quot;&quot;)message(STATUS &quot;ANDROID_ABI=$&#123;ANDROID_ABI&#125;&quot;)#通过在Build.gradle中配置ndk abi filter,默认会全部编译find_package(OpenCV REQUIRED COMPONENTS $&#123;ANDROID_OPENCV_COMPONENTS&#125;)file(GLOB srcs *.cpp *.c)file(GLOB hdrs *.hpp *.h)include_directories(&quot;$&#123;CMAKE_CURRENT_LIST_DIR&#125;&quot;)add_library($&#123;target&#125; SHARED $&#123;srcs&#125; $&#123;hdrs&#125;)target_link_libraries($&#123;target&#125; $&#123;ANDROID_OPENCV_COMPONENTS&#125;) 2.2.3 Native以及JNI编写此处无特别注意之处，均为标准规范 3. Face-detection调整为library3.1 通过Camera2接口+ImageReader接口获取相机帧数据，通过OpenCV SDK中官方代码，将YUV420_888转换成为Mat 3.2 通过OpenCV对Java接口对Mat数据进行旋转等操作，保证送入识别接口的Mat(灰度)方向正常 3.3 此处相机分为前置后置，此处灵活处理 4 总结4.1 我们可以将OpenCV已编译好的so库以及include头文件单独迁移出来，直接使用Native.load()方法直接调用。此方法可以加快构建工程速度，无需再次编译libopencv_java4.so4.2 OpenCV FaceDetection中Nativce方法传递参数时，传递的为Mat索引，此方法减少内存以及快速将Java层Mat传递给Nativce.12345678910111213141516171819202122232425262728293031323334 //第一个参数为灰度Mat,传递给Native的为Long型地址 public void detect(Mat imageGray, MatOfRect faces) &#123; nativeDetect(mNativeObj, imageGray.getNativeObjAddr(), faces.getNativeObjAddr()); &#125;JNIEXPORT void JNICALL Java_org_opencv_samples_facedetect_DetectionBasedTracker_nativeDetect(JNIEnv * jenv, jclass, jlong thiz, jlong imageGray, jlong faces)&#123; //LOGD(&quot;Java_org_opencv_samples_facedetect_DetectionBasedTracker_nativeDetect&quot;); try &#123; vector&lt;Rect&gt; RectFaces; //*((Mat*)imageGray) 此处将地址索引再次转为Mat ((DetectorAgregator*)thiz)-&gt;tracker-&gt;process(*((Mat*)imageGray)); ((DetectorAgregator*)thiz)-&gt;tracker-&gt;getObjects(RectFaces); *((Mat*)faces) = Mat(RectFaces, true); &#125; catch(const cv::Exception&amp; e) &#123; LOGD(&quot;nativeCreateObject caught cv::Exception: %s&quot;, e.what()); jclass je = jenv-&gt;FindClass(&quot;org/opencv/core/CvException&quot;); if(!je) je = jenv-&gt;FindClass(&quot;java/lang/Exception&quot;); jenv-&gt;ThrowNew(je, e.what()); &#125; catch (...) &#123; LOGD(&quot;nativeDetect caught unknown exception&quot;); jclass je = jenv-&gt;FindClass(&quot;java/lang/Exception&quot;); jenv-&gt;ThrowNew(je, &quot;Unknown exception in JNI code DetectionBasedTracker.nativeDetect()&quot;); &#125; //LOGD(&quot;Java_org_opencv_samples_facedetect_DetectionBasedTracker_nativeDetect END&quot;);&#125; 4.3 如果其他Native库使用OpenCV，我们在传入灰度数据时，也可以将Native设计为Mat地址索引4.4 OpenCV FaceDetector可以跟其他人脸Landmark库配合使用，例如：FaceAlignmentSeeta，dlib等，在进行人脸关键点识别时，均需要传递为灰度Mat以及需要人脸框","categories":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://example.com/categories/OpenCV/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://example.com/tags/OpenCV/"}]},{"title":"OpenGLES_FBO使用","slug":"Android/OpenGLES/OpenGLES_FBO","date":"2022-01-07T01:48:00.000Z","updated":"2022-01-09T02:22:35.645Z","comments":true,"path":"2022/01/07/Android/OpenGLES/OpenGLES_FBO/","link":"","permalink":"http://example.com/2022/01/07/Android/OpenGLES/OpenGLES_FBO/","excerpt":"","text":"基本知识 1. Android系统默认渲染器 OpenGL,系统启动时，经过BootLoader启动，kernel启动----&gt;init进程启动核心进程（ServiceManager,zygote,OpenGL)----&gt;播放开机动画 OpenGL渲染管线的最后一个阶段就是帧缓冲区（FrameBuffer) 2. OpenGL渲染管线的最后阶段FrameBuffer,Android系统存在默认缓冲区（window-system-provided frame),用于屏幕显示。GPU往显示缓冲区写入数据时，屏幕会显示缓冲内容。 使用FBO可以让数据不渲染到屏幕上，渲染到离屏的Buffer中 Android中后台给视频添加水印，Camera实时滤镜，需要把原数据经过处理后保存但是不显示数据 3.对于默认的相机，使用系统提供的OpenGL渲染的帧缓冲区（OES），OpenGL所有渲染结果直接到达帧缓冲区----&gt;on-Srceen渲染方式 对于贴纸相机，使用帧缓冲区对象，OpenGL将提供给窗口的帧缓冲区重定向FBO之中 4. FBO提供缓冲区：颜色缓冲区，深度缓冲区，模板缓冲区 FBO提供2种绑定的对象：纹理图片(texture images) 和 渲染图像(renderbuffer images) 4.1 纹理绑定FBO,OpenGL执行渲染到纹理操作 4.2 渲染绑定FBO，OpenGL执行离屏渲染 4.3 通过GL_MAX_COLOR_ATTACHMENTS查询颜色缓冲区挂节点 4.4 纹理对象 glFramebufferTexture2D 渲染对象 glFramebufferRenderbuffer 5. FBO (Frame Buffer Object) 帧缓冲区对象，FBO本身不能用于渲染，只有添加了纹理或者渲染缓冲区后才能作为渲染目标 6. FBO 使用流程图 使用1.搭建基本OpenGLES FBO 仅输出 纹理对象1.1 12protected int[] mFrameBuffers;protected int[] mFrameBufferTextures; 1.2123456789101112131415161718192021GlUtil.checkGlError(TAG, &quot;[onReady()][Start]&quot;);super.onReady(width, height);if (mFrameBuffers != null) &#123; destroyFrameBuffers();&#125;mFrameBuffers = new int[1];//1 创建fboGLES20.glGenFramebuffers(mFrameBuffers.length, mFrameBuffers, 0);mFrameBufferTextures = new int[1];//2 创建fbo纹理GlUtil.glConfigureTextures(mFrameBufferTextures);//3 绑定纹理GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, mFrameBufferTextures[0]);GLES20.glTexImage2D(GLES20.GL_TEXTURE_2D, 0, GLES20.GL_RGBA, mOutputWidth, mOutputHeight, 0, GLES20.GL_RGBA, GLES20.GL_UNSIGNED_BYTE, null);//4 fbo绑定纹理GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, mFrameBuffers[0]);GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, mFrameBufferTextures[0], 0);//5 解绑GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, 0);GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, 0);GlUtil.checkGlError(TAG, &quot;[onReady()][End]&quot;); 2.使用FBO处理图片滤镜(自己创建FrameBuffer,同时输出到纹理对象，渲染对象)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public void createEnvi() &#123; Log.d(TAG, &quot;[createEnvi]&quot;); //创建帧缓冲对象 GLES20.glGenFramebuffers(1, fFrame, 0); //创建渲染缓冲对象 GLES20.glGenRenderbuffers(1, fRender, 0); //相似地，我们打算把渲染缓冲对象绑定，这样所有后续渲染缓冲操作都会影响到当前的渲染缓冲对象 GLES20.glBindRenderbuffer(GLES20.GL_RENDERBUFFER, fRender[0]); //创建一个深度和模板渲染缓冲对象 GLES20.glRenderbufferStorage(GLES20.GL_RENDERBUFFER, GLES20.GL_DEPTH_COMPONENT16, mBitmap.getWidth(), mBitmap.getHeight()); //附加帧缓冲 渲染缓冲对象附加到帧缓冲的深度和模板附件 GLES20.glFramebufferRenderbuffer(GLES20.GL_FRAMEBUFFER, GLES20.GL_DEPTH_ATTACHMENT, GLES20.GL_RENDERBUFFER, fRender[0]); //解绑渲染缓冲 GLES20.glBindRenderbuffer(GLES20.GL_RENDERBUFFER, 0); //fTexture size 2 //fTexture[0] 普通纹理（由bitmap转成) //fTexture[1] 帧缓冲纹理(GLES20.glTextImage2D) GLES20.glGenTextures(2, fTexture, 0); for (int i = 0; i &lt; 2; i++) &#123; GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, fTexture[i]); if (i == 0) &#123; GLUtils.texImage2D(GLES20.GL_TEXTURE_2D, 0, GLES20.GL_RGBA, mBitmap, 0); &#125; else &#123;//纹理的维度设置为图片的大小,传递null作为纹理的data,只分配内存，不写入数据 GLES20.glTexImage2D(GLES20.GL_TEXTURE_2D, 0, GLES20.GL_RGBA, mBitmap.getWidth(), mBitmap.getHeight(), 0, GLES20.GL_RGBA, GLES20.GL_UNSIGNED_BYTE, null); &#125; GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_NEAREST); GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR); GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE); GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE); &#125; mBuffer = ByteBuffer.allocate(mBitmap.getWidth() * mBitmap.getHeight() * 4);&#125; //绑定执行GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fFrame[0]);//将创建好的帧缓冲纹理附加到帧缓冲 纹理对象GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, fTexture[1], 0);//FBO挂接 渲染对象 Renderbuffer 渲染缓冲 对象的一大优点是，它以OpenGL原生渲染格式储存它的数据，因此在离屏渲染到帧缓冲的时候，这些数据就相当于被优化过的了GLES20.glFramebufferRenderbuffer(GLES20.GL_FRAMEBUFFER, GLES20.GL_DEPTH_ATTACHMENT, GLES20.GL_RENDERBUFFER, fRender[0]);GLES20.glViewport(0, 0, mBitmap.getWidth(), mBitmap.getHeight());mFilter.setTextureId(fTexture[0]);mFilter.draw(); GLES20.glReadPixels(0, 0, mBitmap.getWidth(), mBitmap.getHeight(), GLES20.GL_RGBA, GLES20.GL_UNSIGNED_BYTE, mBuffer);if (mCallback != null) &#123; mCallback.onCall(mBuffer);&#125;deleteEnvi();mBitmap.recycle(); 3. 使用FBO处理相机纹理3.1获取相机纹理后，最后返回步骤1中生成的mFrameBufferTextures[0]，即可对相机纹理进行多个滤镜或者变换处理123456789101112131415161718192021GlUtil.checkGlError(TAG, &quot;[onDrawFrame()][Start]&quot;);GLES20.glViewport(left_margin, 0, mOutputWidth, mOutputHeight);GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, mFrameBuffers[0]);GLES20.glUseProgram(mGLProgramId);mGLVertexBuffer.position(0);GLES20.glVertexAttribPointer(vPosition, 2, GLES20.GL_FLOAT, false, 0, mGLVertexBuffer);GLES20.glEnableVertexAttribArray(vPosition);mGLTextureBuffer.position(0);GLES20.glVertexAttribPointer(vCoord, 2, GLES20.GL_FLOAT, false, 0, mGLTextureBuffer);GLES20.glEnableVertexAttribArray(vCoord);GLES20.glUniformMatrix4fv(vMatrix, 1, false, matrix, 0);GLES20.glActiveTexture(GLES20.GL_TEXTURE0);GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, textureId);GLES20.glUniform1i(vTexture, 0);GLES20.glDrawArrays(GLES20.GL_TRIANGLE_STRIP, 0, 4);GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, 0);GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, 0);GlUtil.checkGlError(TAG, &quot;[onDrawFrame()][Start]&quot;);return mFrameBufferTextures[0];//返回创建好的纹理对象-----&gt;进行后续的操作 4.释放12345678910111213141516171819 private void deleteEnvi() &#123; GLES20.glDeleteTextures(2, fTexture, 0); GLES20.glDeleteRenderbuffers(1, fRender, 0); GLES20.glDeleteFramebuffers(1, fFrame, 0); &#125;private void destroyFrameBuffers() &#123; if (mFrameBufferTextures != null) &#123; GLES20.glDeleteTextures(1, mFrameBufferTextures, 0); mFrameBufferTextures = null; &#125; if (mFrameBuffers != null) &#123; GLES20.glDeleteFramebuffers(1, mFrameBuffers, 0); mFrameBuffers = null; &#125; &#125; 5.参考 MrYeLiang Android-OpenGL-Filter 相机纹理3次FBO纹理变化 湖广午王 AndroidOpenGLDemo FBO 图片灰度滤镜 6.Camera2+GLSurfaceView+FBO","categories":[{"name":"OpenGLES Android Shader","slug":"OpenGLES-Android-Shader","permalink":"http://example.com/categories/OpenGLES-Android-Shader/"}],"tags":[{"name":"OpenGLES","slug":"OpenGLES","permalink":"http://example.com/tags/OpenGLES/"}]},{"title":"Camera2+GlSurfaceView","slug":"Android/Camera2/Camera2_GlSurfaceView","date":"2022-01-07T01:46:14.000Z","updated":"2022-01-08T08:38:26.681Z","comments":true,"path":"2022/01/07/Android/Camera2/Camera2_GlSurfaceView/","link":"","permalink":"http://example.com/2022/01/07/Android/Camera2/Camera2_GlSurfaceView/","excerpt":"","text":"基本概念:屏幕方向 ScreenOrientation:0度使用Android的GLSurfaceView显示时，得出结论：摄像头后置：摄像头ID “0”Surface.ROTATION_0 画面需要逆时针旋转90度，同时调整GLSurfaceView的显示比例Surface.ROTATION_90 画面方向无需处理Surface.ROTATION_180 画面方向无需处理Surface.ROTATION_270 湖面需要顺时针旋转180度，同时调整GLSurfaceView的显示比例 Camera2打开相机1.仅用来显示：获取当前显示屏幕的DisplayWidth,DisplayHeight(屏幕的宽高) 2.仅用来显示：获取需要打开摄像头的Preview的Size属性(cameraCharacteristics.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP)),获取其中最符合屏幕比例且分辨率大的PreViewSize 3.通过CameraManager.OpenCamera()方法打开相机设置 4.通过CaptureRequest.Builde设置相机的Target(可以设置多个) 例如：通过GLES生成OES纹理并绑定到SurfaceTexture上，通过并将SurfaceTexture设置宽高(1,2步骤获取)后，添加到Surface上，最后将Surface添加到 CaptureRequest.Builde.addTarget(surface); 例如：创建ImageReader，并将其Surface(getSurface()),添加到CaptureRequest.Builde.addTarget(surface); 结论：CaptureRequest.Builde.addTarget(Surface)方法的意义在于将目标Surface(提前设置宽高)传递给相机，相机生产方可以同时将不同分辨率的数据传递给不同消费者。 消费者：TextureView,GLSurfaceView,ImageReader等 显示相机画面1.通过GLSurfaceView显示相机画面(绑定OES纹理，自定义GLSurfaceView.Renderer)即可，画面方向以及比例可以通过顶点坐标或者相机Matrix来进行调节 2.通过TextureView显示相机画面，将TextureView.getSurfaceTexture()方法，设置target同理，可以对TextureView进行旋转，平移操作，保证画面比例正常 ImageReader创建1.ImageReader的可以用来获取相机的帧数据，通过对帧数据的处理，实现对象检测,人脸检测,等需要单帧处理的场景 2.在很多官方例子中便可以看到，例如： 2.1 TFLite ObjectDetect 中将Image数据转成Bitmap然后传递给识别接口 2.2 OpenCV Face Detect中将Image数据转成Mat,然后将灰度Mat传递给识别接口 2.3 人脸识别以及landmark中通过将Image数据转换成为NV21类型，传递给识别接口 3. 显示相机画面时部分Orientation进行旋转操作，同理ImageReader获取到的接口也需要进行处理，方可得到正确方向的数据。 ImageReader转码 OpenCV转码(来源于OpenCV SDK官方实现 ImageFormat.YUV_420_888转Mat) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public class Yuv4208882Mat implements CvCameraViewFrame &#123; public Mat gray() &#123; Image.Plane[] planes = mImage.getPlanes(); int w = mImage.getWidth(); int h = mImage.getHeight(); assert (planes[0].getPixelStride() == 1); ByteBuffer y_plane = planes[0].getBuffer(); int y_plane_step = planes[0].getRowStride(); mGray = new Mat(h, w, CvType.CV_8UC1, y_plane, y_plane_step); return mGray; &#125; public Mat rgba() &#123; Image.Plane[] planes = mImage.getPlanes(); int w = mImage.getWidth(); int h = mImage.getHeight(); int chromaPixelStride = planes[1].getPixelStride(); if (chromaPixelStride == 2) &#123; // Chroma channels are interleaved assert (planes[0].getPixelStride() == 1); assert (planes[2].getPixelStride() == 2); ByteBuffer y_plane = planes[0].getBuffer(); int y_plane_step = planes[0].getRowStride(); ByteBuffer uv_plane1 = planes[1].getBuffer(); int uv_plane1_step = planes[1].getRowStride(); ByteBuffer uv_plane2 = planes[2].getBuffer(); int uv_plane2_step = planes[2].getRowStride(); Mat y_mat = new Mat(h, w, CvType.CV_8UC1, y_plane, y_plane_step); Mat uv_mat1 = new Mat(h / 2, w / 2, CvType.CV_8UC2, uv_plane1, uv_plane1_step); Mat uv_mat2 = new Mat(h / 2, w / 2, CvType.CV_8UC2, uv_plane2, uv_plane2_step); long addr_diff = uv_mat2.dataAddr() - uv_mat1.dataAddr(); if (addr_diff &gt; 0) &#123; assert (addr_diff == 1); Imgproc.cvtColorTwoPlane(y_mat, uv_mat1, mRgba, Imgproc.COLOR_YUV2RGBA_NV12); &#125; else &#123; assert (addr_diff == -1); Imgproc.cvtColorTwoPlane(y_mat, uv_mat2, mRgba, Imgproc.COLOR_YUV2RGBA_NV21); &#125; return mRgba; &#125; else &#123; // Chroma channels are not interleaved byte[] yuv_bytes = new byte[w * (h + h / 2)]; ByteBuffer y_plane = planes[0].getBuffer(); ByteBuffer u_plane = planes[1].getBuffer(); ByteBuffer v_plane = planes[2].getBuffer(); int yuv_bytes_offset = 0; int y_plane_step = planes[0].getRowStride(); if (y_plane_step == w) &#123; y_plane.get(yuv_bytes, 0, w * h); yuv_bytes_offset = w * h; &#125; else &#123; int padding = y_plane_step - w; for (int i = 0; i &lt; h; i++) &#123; y_plane.get(yuv_bytes, yuv_bytes_offset, w); yuv_bytes_offset += w; if (i &lt; h - 1) &#123; y_plane.position(y_plane.position() + padding); &#125; &#125; assert (yuv_bytes_offset == w * h); &#125; int chromaRowStride = planes[1].getRowStride(); int chromaRowPadding = chromaRowStride - w / 2; if (chromaRowPadding == 0) &#123; // When the row stride of the chroma channels equals their width, we can copy // the entire channels in one go u_plane.get(yuv_bytes, yuv_bytes_offset, w * h / 4); yuv_bytes_offset += w * h / 4; v_plane.get(yuv_bytes, yuv_bytes_offset, w * h / 4); &#125; else &#123; // When not equal, we need to copy the channels row by row for (int i = 0; i &lt; h / 2; i++) &#123; u_plane.get(yuv_bytes, yuv_bytes_offset, w / 2); yuv_bytes_offset += w / 2; if (i &lt; h / 2 - 1) &#123; u_plane.position(u_plane.position() + chromaRowPadding); &#125; &#125; for (int i = 0; i &lt; h / 2; i++) &#123; v_plane.get(yuv_bytes, yuv_bytes_offset, w / 2); yuv_bytes_offset += w / 2; if (i &lt; h / 2 - 1) &#123; v_plane.position(v_plane.position() + chromaRowPadding); &#125; &#125; &#125; Mat yuv_mat = new Mat(h + h / 2, w, CvType.CV_8UC1); yuv_mat.put(0, 0, yuv_bytes); Imgproc.cvtColor(yuv_mat, mRgba, Imgproc.COLOR_YUV2RGBA_I420, 4); return mRgba; &#125; &#125; public Yuv4208882Mat(Image image) &#123; super(); mImage = image; mRgba = new Mat(); mGray = new Mat(); &#125; public void release() &#123; mRgba.release(); mGray.release(); &#125; private Image mImage; private Mat mRgba; private Mat mGray;&#125; 例子TFLite Object Detector(ImageFormat.YUV_420_888转Bitmap) 1234567891011121314151617181920212223242526272829303132333435363738private void saveBitmapFromBytes(Image image) &#123; Image.Plane[] planes = image.getPlanes(); byte[][] yuvBytes = new byte[3][]; fillBytes(planes, yuvBytes); int yRowStride = planes[0].getRowStride(); int uvRowStride = planes[1].getRowStride(); int uvPixelStride = planes[1].getPixelStride(); ImageUtils.convertYUV420ToARGB8888( yuvBytes[0], yuvBytes[1], yuvBytes[2], mPreviewSize.getWidth(), mPreviewSize.getHeight(), yRowStride, uvRowStride, uvPixelStride, rgbBytes); Bitmap rgbFrameBitmap = Bitmap.createBitmap(mPreviewSize.getWidth(), mPreviewSize.getHeight(), Bitmap.Config.ARGB_8888); rgbFrameBitmap.setPixels(rgbBytes, 0, mPreviewSize.getWidth(), 0, 0, mPreviewSize.getWidth(), mPreviewSize.getHeight()); File file = new File(activity.getExternalCacheDir() + &quot;/&quot; + System.currentTimeMillis() + &quot;_compress.jpeg&quot;); if (!file.exists()) &#123; file.getParentFile().mkdirs(); try &#123; file.createNewFile(); &#125; catch (IOException e) &#123; e.printStackTrace(); Logger.e(TAG, &quot;[imageAvailableListenerDetect]&quot;); return; &#125; &#125; try &#123; rgbFrameBitmap.compress(Bitmap.CompressFormat.JPEG, 10, new FileOutputStream(file)); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; 例子3 部分人脸识别(ImageFormat.YUV_420_888转NV21) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public static byte[] getDataFromImage(Image image, int colorFormat) &#123; if (colorFormat != COLOR_FormatI420 &amp;&amp; colorFormat != COLOR_FormatNV21) &#123; throw new IllegalArgumentException(&quot;only support COLOR_FormatI420 &quot; + &quot;and COLOR_FormatNV21&quot;); &#125; if (!isImageFormatSupported(image)) &#123; throw new RuntimeException(&quot;can&#x27;t convert Image to byte array, format &quot; + image.getFormat()); &#125; Rect crop = image.getCropRect(); int format = image.getFormat(); int width = crop.width(); int height = crop.height(); Image.Plane[] planes = image.getPlanes(); byte[] data = new byte[width * height * ImageFormat.getBitsPerPixel(format) / 8]; byte[] rowData = new byte[planes[0].getRowStride()]; int channelOffset = 0; int outputStride = 1; for (int i = 0; i &lt; planes.length; i++) &#123; switch (i) &#123; case 0: channelOffset = 0; outputStride = 1; break; case 1: if (colorFormat == COLOR_FormatI420) &#123; channelOffset = width * height; outputStride = 1; &#125; else if (colorFormat == COLOR_FormatNV21) &#123; channelOffset = width * height + 1; outputStride = 2; &#125; break; case 2: if (colorFormat == COLOR_FormatI420) &#123; channelOffset = (int) (width * height * 1.25); outputStride = 1; &#125; else if (colorFormat == COLOR_FormatNV21) &#123; channelOffset = width * height; outputStride = 2; &#125; break; &#125; ByteBuffer buffer = planes[i].getBuffer(); int rowStride = planes[i].getRowStride(); int pixelStride = planes[i].getPixelStride(); int shift = (i == 0) ? 0 : 1; int w = width &gt;&gt; shift; int h = height &gt;&gt; shift; buffer.position(rowStride * (crop.top &gt;&gt; shift) + pixelStride * (crop.left &gt;&gt; shift)); for (int row = 0; row &lt; h; row++) &#123; int length; if (pixelStride == 1 &amp;&amp; outputStride == 1) &#123; length = w; buffer.get(data, channelOffset, length); channelOffset += length; &#125; else &#123; length = (w - 1) * pixelStride + 1; buffer.get(rowData, 0, length); for (int col = 0; col &lt; w; col++) &#123; data[channelOffset] = rowData[col * pixelStride]; channelOffset += outputStride; &#125; &#125; if (row &lt; h - 1) &#123; buffer.position(buffer.position() + rowStride - length); &#125; &#125; &#125; return data;&#125; 总结：Image的转码核心在于提取Y,U,V3个通道的数据后按照规定数据进行转换或者重新排列即可 参照例子 OpenCV Android SDK 4.5.2 (自行下载使用) TFLite Object Detect","categories":[{"name":"Android Camera2 OpenGLES ImageReader","slug":"Android-Camera2-OpenGLES-ImageReader","permalink":"http://example.com/categories/Android-Camera2-OpenGLES-ImageReader/"}],"tags":[{"name":"Camera2","slug":"Camera2","permalink":"http://example.com/tags/Camera2/"}]},{"title":"java_String源码学习","slug":"java/lang/String","date":"2020-12-15T15:11:08.000Z","updated":"2022-01-17T03:13:59.377Z","comments":true,"path":"2020/12/15/java/lang/String/","link":"","permalink":"http://example.com/2020/12/15/java/lang/String/","excerpt":"","text":"1. Char1. String 底层实现为 char[]-----&gt;private final char value[]//char[] 数组 2. String类为final类，也就是无法通过子类去继承。（final修饰的方法无法复写) 2. String2.1 String 实现了接口列表 1.1 java.io.Serialliable 序列化 1.2 Comparable&lt;String&gt;----&gt;public int compareTo(String) 此接口用来设置排序的规则 1.3 CharSequence 2.2 String 内部实现 2.1 private final char value[];//使用字符数组进行存储 2.2 private int hash;//缓存字符串的哈希值 2.3 String 方法 3.1 String的构造方法中，涉及charset，使用到了StringCode类以及使用Arrays.copyof() 通过对char数组进行拷贝 3. StringBuilder3.1. StringBuilderA mutable sequence of characters.（可变序列序列），but with no guarantee of synchronization(但是无法保证同步)，used by a single thread,it will be faster under most implementation. 3.2 StringBuilder 同样为final类，无法通过子类继承去修改实现,继承AbstractStringBuilder,实现的接口列表 3.2.1 java.io.Serializable 序列化 3.2.2 CharSequence 继承AbstartcStringBuilder抽象类 3.2.3 数据使用char[] value,默认实现大小16 3.2.4 StringBuilder 方法中大部分实现均通过super关键字，使用父类AbstractStringBuilder父类实现 4. StringBuffer4.1 StringBufferA thread -safe,mutalble sequence of characrters,A String Buffer is like a String ,but can be modified,线程安全的可变字符序列 4.2 final类，继承AbstarctStringBuilder,实现Serializablel以及CharSequence接口4.2.1 线程安全体现的所有的public方法均使用 synchronized 同步关键字来进行保证 4.2.2 toStringCache 使用 trasition 关键字修饰，禁止序列化，同时每次方法调用时都会将其置为空，同时toString()方法调用时，会通过Arrays.copyOfRange来进行copy 5 参照源码 6.延申知识### 1. java.lang.Comparable的使用 o:specified object return: a negative integer less than zero equal to a positive integer greater than public int compareTo(T o); 查看String的源码发现实现此接口，String的比大小规则:两个字符串均从第一个字符起，比较字符的Unicode Value的大小 123456789101112131415161718public int compareTo(String anotherString) &#123; int len1 = value.length; int len2 = anotherString.value.length; int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; while (k &lt; lim) &#123; char c1 = v1[k]; char c2 = v2[k]; if (c1 != c2) &#123; return c1 - c2; &#125; k++; &#125; return len1 - len2;&#125; ### 2. 其他地方的使用 Lists (and arrays) of objects that implement this interface can be sorted automatically by Collection#sort(list) Collections.sort And Arrays.srot(Object[]) Arrays.sort. 这里提供集合对象的排序可以自动使用2个实现。 Collections.class 1234public static &lt;T extends Comparable&lt;? super T&gt;&gt; void sort(List&lt;T&gt; list) &#123; list.sort(null);&#125; List.class 1234567891011@SuppressWarnings(&#123;&quot;unchecked&quot;, &quot;rawtypes&quot;&#125;)default void sort(Comparator&lt;? super E&gt; c) &#123; Object[] a = this.toArray(); Arrays.sort(a, (Comparator) c); ListIterator&lt;E&gt; i = this.listIterator(); for (Object e : a) &#123; i.next(); i.set((E) e); &#125;&#125; Arrays.class ``` public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c) &#123; if (c == null) &#123; sort(a); &#125; else &#123; if (LegacyMergeSort.userRequested) legacyMergeSort(a, c); else TimSort.sort(a, 0, a.length, c, null, 0, 0); &#125; &#125; ``` ### 3.CharSequence 3.1简介 &lt;tt&gt;CharSequence&lt;/tt&gt; is a readable sequence of &lt;code&gt;char&lt;/code&gt; values. This interface provides uniform, read-only access to many different kinds of &lt;code&gt;char&lt;/code&gt; sequences. 此接口提供对于char的可读序列，可以用来提供统一，仅读 对于各种各样的char序列(String,StringBuilder,StringBuffer)","categories":[{"name":"Java源码","slug":"Java源码","permalink":"http://example.com/categories/Java%E6%BA%90%E7%A0%81/"}],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"http://example.com/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Java集合结构学习","slug":"java/util/java集合相关","date":"2020-12-13T12:41:02.000Z","updated":"2022-01-08T08:37:43.448Z","comments":true,"path":"2020/12/13/java/util/java集合相关/","link":"","permalink":"http://example.com/2020/12/13/java/util/java%E9%9B%86%E5%90%88%E7%9B%B8%E5%85%B3/","excerpt":"","text":"源码类图 源码学习1.List1.1 Vector 在线查看 Vector实现是数组，默认空构造方法长度为10，也可以根据传入initialCapacity 12345678910111213public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125;public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; Vector使用时 允许添加value是null size(),isEmpty(),get(index),set(int,E),add() 方法的执行时间复杂度O(1),注意这里的关键字Synchronized 1234567891011121314151617181920212223242526272829public synchronized int size() &#123; return elementCount;&#125;public synchronized boolean isEmpty() &#123; return elementCount == 0;&#125; public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125;public synchronized E set(int index, E element) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 线程是否安全？ 肯定的哈，这么多synchronized 扩容:看到这里发现了个”+”,原来是2倍 1234567891011grow(minCapacity);private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; 扩容本质: 1Arrays.copyOf(elementData, newCapacity) Vector看起来平常很少用哈！！！ 1.2 ArrayList 在线查看 ArrayList实现是一个数组，默认长度10，size（int）表示长度。代码摘选如下： 123private static final int DEFAULT_CAPACTITY=10;//默认长度transient Object[] elementData;//基本实现数组private int size;//大小 ArrayList 使用时 允许添加value是null的值 size(),isEmpty(),get(index),set(),add()方法的执行时间复杂度均为O(1),这里的时间复杂度表示访问单个元素时，因为访问只需要一条指令。 12345678910111213141516171819202122232425262728293031//返回List的大小实现public int size() &#123;return size;&#125;//返回List是否为空public boolean isEmpty() &#123; return size == 0;&#125;//根据index返回valuepublic E get(int index) &#123; rangeCheck(index); // 检查是否越界 return elementData(index);// 返回ArrayList的elementData数组index位置的元素&#125;//设置index的valuepublic E set(int index, E element) &#123; rangeCheck(index); // 检查是否越界 E oldValue = elementData(index);// 调用elementData(index)获取到当前位置的 elementData[index] = element; // 将element赋值到ArrayList的elementData数组的第index位置 return oldValue;&#125;//添加一个值(在添加时未进行null判断，所有ArrayLsit是允许空值存在)public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // 扩容 // Increments modCount!! elementData[size++] = e; // 将e赋值给elementData的size+1的位置 return true;&#125; 线程是否安全？ 不安全，这里并未看到对方法或者全局变量使用synchronized关键字。以及size并未使用volatile修饰 扩容规则 扩容大小为原大小的1.5倍,后使用Arrays.copyof(old,new) 123456789private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; // 获取到ArrayList中elementData数组的内存空间长度 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);// 扩容至原来的1.5倍 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0)//若预设值大于默认的最大值检查是否溢出 newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);// 并将elementData的数据复制到新的内存空间&#125; 扩容的本质： 1System.arraycopy(elementData, 0, newElementData, 0,Math.min(elementData.length,newCapacity)) 使用总结 ArrayList删除时间复杂度为O(1),且允许删除null值 ArrayList全局变量且多线程使用时，存在问题.作为局部变量，放心用吧！！！ 可以使用的Collections#synchronizedList保证线程安全 使用ArrayList时为了便面多次copy的过程,可以在初始化时指定固定大小 1.2 LinkedList 在线查看 LinkedList底层数据结构是一个双向链表，既然是双向链表。可以被当作栈，队列。非线程安全 123transient int size=0;transient Node&lt;E&gt; first;//transient 防止序列化transient Node&lt;E&gt; last; LinkedList 方法实现(这里只对思路和public方法记录下) 头节点插入 1234567891011private void linkFirst(E e)&#123; final Node&lt;E&gt; f = first; final Node&lt;E&gt; newNode = new Node&lt;&gt;(null,e,f); first = newNode; if(f == null) last = newNode; else f.prev = newNode; size++; modCount++; &#125; 尾节点插入(不做笔记) 在succ节点前插入 1234567891011void linkBefore(E e,Node&lt;E&gt; succ)&#123; final Node&lt;E&gt; pred = succ.prev;//保存前一个节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred,e,succ);//创建一个新的节点 succ.prev = newNode;//succ的前节点设置为newNode if(pred==null) first = newNode; else pred.next = newNode; size++; modCount++; &#125; 删除非空节点 获取首节点,并且返回头节点值 123456public E getFirst()&#123; final Node&lt;E&gt; f = first; if(f==null) throw new NoSuchElementException(); return f.item &#125; 获取尾节点，返回存储的元素值 在头节点插入 判断元素是否包含 12345 public boolean contains(Object o)&#123; return indexOf(o)!=-1; &#125; ``` - 查询操作返回对应的Index public int indexOf(Object o)&#123; int index = 0; if(o==null)&#123; for(Node&lt;E&gt; x= first;x!=null;x=x.next)&#123; if(x.itme==null) return index; index++; &#125; &#125;else&#123; for(Node&lt;E&gt; x= first;x!=null;x=x.next)&#123; if(o.equals(x.item)) return index; index++; &#125; &#125; &#125; 123456789 - 获取头节点 peeK() - 获取并删除头节点 poll() - 等等（这些方法的实现还是比较简单，都是比较常规的操作)### 2.Map#### 2.1 HashMap- [在线查看](https://github.com/wupeixuan/JDKSourceCode1.8/blob/master/src/java/util/HashMap.java)- HashMap底层数据结构 数组+链表+红黑树，当链表的长度大于等于8时，链表会转换成为红黑树，当红黑树的大小小于等于6时，红黑树会转换成为链表。 static final int DEFAULT_ININIAL_CAPCITY=1&lt;&lt;4;//默认16 static final int MAXIMUM_CAPACITY =1&lt;&lt;30;//最大 transient Node&lt;K,V&gt;[] table;//存储数组的元素 static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt;{}//红黑树的实现 static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt;{}//链表的节点``` 2.1.1 LinkedHashMap2.2 TreeMap2.3 HashTable2.4 EnumMap3.Set3.1 HashSet3.1.1 LinkedHashSet3.2 TreeSetSortedSet","categories":[{"name":"Java源码","slug":"Java源码","permalink":"http://example.com/categories/Java%E6%BA%90%E7%A0%81/"}],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"http://example.com/tags/Java%E6%BA%90%E7%A0%81/"}]},{"title":"Hexo+Github+fluid搭建","slug":"my-site","date":"2020-12-12T11:10:53.000Z","updated":"2022-01-15T08:23:12.016Z","comments":true,"path":"2020/12/12/my-site/","link":"","permalink":"http://example.com/2020/12/12/my-site/","excerpt":"","text":"Hexo+Github+fluid搭建记录主体流程1.Github 注册账号,创建”*.github.io”的仓库2.安装Node.js后,使用npm install hexo3.修改默认theme为fluid 需要参考的链接GithubNode.jsHexoFluid 创建博客的原因 一名Android开发 日常记录以上这些模块曾经已经整理详细学习记录（软件&amp;硬件知识)，希望在后续维护中，补充上来！！！ 其他知识模块如下 2.1 Android Camera(相机应用) 2.2 Android OpenGLES(图像处理) 2.3 Android AI(TFLite) 学习方向 3.1人脸算法了解和学习 3.2 算法题","categories":[{"name":"其他","slug":"其他","permalink":"http://example.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"工具使用记录","slug":"工具使用记录","permalink":"http://example.com/tags/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-12-12T11:04:53.223Z","updated":"2020-12-12T11:04:53.223Z","comments":true,"path":"2020/12/12/hello-world/","link":"","permalink":"http://example.com/2020/12/12/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"Java源码","slug":"Java源码","permalink":"http://example.com/categories/Java%E6%BA%90%E7%A0%81/"},{"name":"OpenGLES MediaCodec","slug":"OpenGLES-MediaCodec","permalink":"http://example.com/categories/OpenGLES-MediaCodec/"},{"name":"Android Zxing","slug":"Android-Zxing","permalink":"http://example.com/categories/Android-Zxing/"},{"name":"OpenGLES Android Shader","slug":"OpenGLES-Android-Shader","permalink":"http://example.com/categories/OpenGLES-Android-Shader/"},{"name":"Android Camera2 APP","slug":"Android-Camera2-APP","permalink":"http://example.com/categories/Android-Camera2-APP/"},{"name":"FaceLandmark OpenCV","slug":"FaceLandmark-OpenCV","permalink":"http://example.com/categories/FaceLandmark-OpenCV/"},{"name":"OpenCV","slug":"OpenCV","permalink":"http://example.com/categories/OpenCV/"},{"name":"Android Camera2 OpenGLES ImageReader","slug":"Android-Camera2-OpenGLES-ImageReader","permalink":"http://example.com/categories/Android-Camera2-OpenGLES-ImageReader/"},{"name":"其他","slug":"其他","permalink":"http://example.com/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"Java源码","slug":"Java源码","permalink":"http://example.com/tags/Java%E6%BA%90%E7%A0%81/"},{"name":"MediaCodec","slug":"MediaCodec","permalink":"http://example.com/tags/MediaCodec/"},{"name":"Zxing","slug":"Zxing","permalink":"http://example.com/tags/Zxing/"},{"name":"OpenGLES","slug":"OpenGLES","permalink":"http://example.com/tags/OpenGLES/"},{"name":"APP","slug":"APP","permalink":"http://example.com/tags/APP/"},{"name":"FaceLandmark","slug":"FaceLandmark","permalink":"http://example.com/tags/FaceLandmark/"},{"name":"SeetaFace","slug":"SeetaFace","permalink":"http://example.com/tags/SeetaFace/"},{"name":"OpenCV","slug":"OpenCV","permalink":"http://example.com/tags/OpenCV/"},{"name":"Camera2","slug":"Camera2","permalink":"http://example.com/tags/Camera2/"},{"name":"工具使用记录","slug":"工具使用记录","permalink":"http://example.com/tags/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"}]}